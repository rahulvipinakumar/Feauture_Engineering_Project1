{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; \n",
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "\n",
    "from utils.year_month import get_year_month\n",
    "from utils.avg_sal import get_avg_sal\n",
    "from utils.daily_avg_sal import get_daily_avg_sal\n",
    "from utils.dropping_feauture import drop_feauture\n",
    "from utils.encoding import one_hot_encoding\n",
    "from utils.gap_filling import gap_filling_by_mode\n",
    "from utils.removing_special_char import remove_special_chars\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-ETL_Final\").getOrCreate() \n",
    "\n",
    "def file_read(filename):\n",
    "    # filename: File name with path to extract data\n",
    "    \n",
    "    return spark.read.options(inferSchema='True').option('escape','\"').csv(filename, header=True)\n",
    "\n",
    "def write_data(df_spark,filename):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # filename : Target file path\n",
    "    df_spark.coalesce(1).write.mode('overwrite').option('header',True).option(\"encoding\", \"UTF-8\").option(\"escape\",\"\\\"\").csv(filename)\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #Creating spark session variable\n",
    "    #extracting data from the provided file\n",
    "    df_spark=file_read(\"/dataset/nyc-jobs.csv\")\n",
    "    \n",
    "    # Categorical Imputation for filling missing 'Full-Time/Part-Time indicator' and 'Posting Date'\n",
    "    df_spark=gap_filling_by_mode(df_spark,['Salary Frequency'],'Full-Time/Part-Time indicator')\n",
    "    df_spark=gap_filling_by_mode(df_spark,['Posting Type','Agency'],'Posting Date')\n",
    "    \n",
    "    # Creating Feature salary_avg for quantifying salary across multiple job titles\n",
    "    df_spark=get_avg_sal(df_spark, \"Salary Range From\",\"Salary Range To\")\n",
    "    \n",
    "    # Creating Feature daily_salary_avg for normalizing salary across multiple job titles irrspective of Salary Frquency\n",
    "    df_spark=get_daily_avg_sal(df_spark, \"salary_avg\",\"Salary Frequency\")\n",
    "    \n",
    "    \"\"\"df_spark=get_year_month(df_spark,\"Posting Date\", \"Job_Post_Year\",\"Job_Post_Month\")\n",
    "    df_spark=get_year_month(df_spark,\"Process Date\", \"data_processed_year\",\"data_processed_month\")\n",
    "    \"\"\"\n",
    "    \n",
    "    #Cleaning data by removing special characters in \"Minimum Qual Requirements\" column\n",
    "    df_spark=remove_special_chars(df_spark,\"Minimum Qual Requirements\")\n",
    "    \n",
    "    #One Hot encoding for Posting Type and Salary Frequency for converting Catgorical features to Numerical features\n",
    "    df_spark=one_hot_encoding(df_spark,'Posting Type')\n",
    "    df_spark=one_hot_encoding(df_spark,'Salary Frequency')\n",
    "    #Reason for using only two categorical columns for One Hot encoding is because they didnt have any null values\n",
    "    # The 'Full-Time/Part-Time indicator' has been edited for removing the NULL values, so didnt use for encoding.\n",
    "    \n",
    "    #Feature Selection by removing the feautures with redundant data like, 'Posting Type','Salary Frequency', 'salary_avg'\n",
    "    #'Recruitment Contact' is dropping as all are null values\n",
    "    df_spark=drop_feauture(df_spark,['Posting Type','Salary Frequency', 'salary_avg', 'Recruitment Contact'])\n",
    "    \n",
    "    #writing data to csv file\n",
    "    write_data(df_spark,\"/dataset/output.csv\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
