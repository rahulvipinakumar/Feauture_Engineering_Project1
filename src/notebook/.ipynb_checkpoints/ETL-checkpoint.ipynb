{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; \n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-ETL\").getOrCreate() \n",
    "\n",
    "def file_read(filename):\n",
    "    # filename: File name with path to extract data\n",
    "    \n",
    "    return spark.read.options(inferSchema='True').option('escape','\"').csv(filename, header=True)\n",
    "    \n",
    "def gap_filling_by_mode(df_spark,grp_by_cols,req_col):\n",
    "    # df_spark : Input spark dataframe where missing data need to be populated\n",
    "    # grp_by_cols : List of columns used for finding the mode of column values, whose null values need to be replaced\n",
    "    # req_col : Column whose null values need to be replaced\n",
    "    \n",
    "    # Creating copy of column lists used for grouping\n",
    "    grp_by_cols_copy=grp_by_cols.copy()\n",
    "    grp_by_cols_copy.append(req_col)\n",
    "    # Finding Mode values of req_col for each group of grp_by_cols values\n",
    "    df_spark_tmp=df_spark.groupby(grp_by_cols_copy).count().orderBy(\"count\", ascending=False)\n",
    "    windowSpec  = Window.partitionBy(grp_by_cols).orderBy(f.col(\"count\").desc())\n",
    "    df_spark_tmp_final=df_spark_tmp.withColumn(\"rank\",f.row_number().over(windowSpec))\n",
    "    # Rank=1 is used for finding the most frequently used value of req_col(mode) for each combinations of grp_by_cols\n",
    "    df_spark_tmp_final=df_spark_tmp_final.filter(f.col(\"rank\") == 1).withColumnRenamed(req_col,req_col+\"_new\")  \n",
    "    #Collecting the column list of input dataframe\n",
    "    main_cols=df_spark.columns\n",
    "    #Joining the Mode dataframe and input dataframe\n",
    "    df_spark=df_spark.join(df_spark_tmp_final, on=grp_by_cols, how='left')\n",
    "    #Replacing NULL values of req_col with Mode value(req_col_new)\n",
    "    df_spark=df_spark.withColumn(req_col,f.coalesce(f.col(req_col),f.col(req_col+\"_new\")))\n",
    "    return df_spark[[main_cols]]\n",
    "    \n",
    "def get_avg_sal(df_spark, sal_from,sal_to):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # sal_from and sal_to are salary ranges for each job positions\n",
    "    \n",
    "    # Will convert the salary columns to Integer type for performing aggregate functions\n",
    "    df_spark = df_spark.withColumn(sal_from,df_spark[sal_from].cast(IntegerType()))\n",
    "    df_spark = df_spark.withColumn(sal_to,df_spark[sal_to].cast(IntegerType()))\n",
    "    \n",
    "    # Finding average salary from from and To salaries\n",
    "    df_spark = df_spark.withColumn(\"salary_avg\",(df_spark[sal_from]+df_spark[sal_to])/2.0)\n",
    "    return df_spark\n",
    "    \n",
    "def get_daily_avg_sal(df_spark, sal_avg):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # sal_avg : Avg salary used for normalizing to daily level\n",
    "    \n",
    "    #Normalizing the salary to a frequency of Daily\n",
    "    df_spark = df_spark.withColumn(\"daily_salary_avg\", when(df_spark[\"Salary Frequency\"] == \"Annual\",df_spark[\"salary_avg\"]/(12*22.00))\\\n",
    "                             .when(df_spark[\"Salary Frequency\"] == \"Hourly\",df_spark[\"salary_avg\"]*8.00)\\\n",
    "                             .otherwise(df_spark[\"salary_avg\"]))\n",
    "    return df_spark\n",
    "  \n",
    "def get_year_month(df_spark,date_col, year_col,month_col):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # sal_avg : Avg salary used for normalizing to daily level\n",
    "    \n",
    "    # column with Year and month details of date_col\n",
    "    df_spark=df_spark.withColumn(year_col,f.year(f.to_timestamp(df_spark[date_col],'yyyy-MM-dd hh24:mi:ss.ff3'))\\\n",
    "                               .cast(IntegerType())).withColumn(month_col,\\\n",
    "                                                                f.month(f.to_timestamp(df_spark[date_col],'yyyy-MM-dd hh24:mi:ss.ff3'))\\\n",
    "                                                                .cast(IntegerType()))\n",
    "    return df_spark\n",
    "    \n",
    "def remove_special_chars(df_spark,col_name):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # col_name : Column value need to be cleaned\n",
    "    \n",
    "    # Cleaning column col_name by reamoving special characters\n",
    "    df_spark=df_spark.withColumn(col_name, f.regexp_replace(df_spark[col_name], \"[^a-zA-Z0-9. ]\", \"\"))\n",
    "    return df_spark\n",
    "    \n",
    "def one_hot_encoding(df_spark,col_name):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # col_name : Column value need to be encoded\n",
    "    \n",
    "    # fetching unique column values\n",
    "    unique_vals =[i[col_name] for i in df_spark.select(col_name).distinct().collect()]\n",
    "    #Assigning 1 for new column based of values of col_name\n",
    "    for j in unique_vals:\n",
    "        df_spark = df_spark.withColumn(j, when(df_spark[col_name] == j,1).otherwise(0))\n",
    "    return df_spark\n",
    "    \n",
    "def drop_feauture(df_spark,col_list):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # col_list : LIST of columns to be removed\n",
    "    \n",
    "    # Removing columns in the given list\n",
    "    for i in col_list:\n",
    "        df_spark=df_spark.drop(i)\n",
    "    return df_spark\n",
    "    \n",
    "def write_data(df_spark,filename):\n",
    "    # df_spark : Input spark dataframe \n",
    "    # filename : Target file path\n",
    "    df_spark.coalesce(1).write.mode('overwrite').option('header',True).option(\"encoding\", \"UTF-8\").option(\"quote\", '\"').csv(filename)\n",
    "    \n",
    "def main():\n",
    "    #Creating spark session variable\n",
    "    #extracting data from the provided file\n",
    "    df_spark=file_read(\"/dataset/nyc-jobs.csv\")\n",
    "    \n",
    "    # Categorical Imputation for filling missing 'Full-Time/Part-Time indicator' and 'Posting Date'\n",
    "    df_spark=gap_filling_by_mode(df_spark,['Salary Frequency'],'Full-Time/Part-Time indicator')\n",
    "    df_spark=gap_filling_by_mode(df_spark,['Posting Type','Agency'],'Posting Date')\n",
    "    \n",
    "    # Creating Feature salary_avg for quantifying salary across multiple job titles\n",
    "    df_spark=get_avg_sal(df_spark, \"Salary Range From\",\"Salary Range To\")\n",
    "    \n",
    "    # Creating Feature daily_salary_avg for normalizing salary across multiple job titles irrspective of Salary Frquency\n",
    "    df_spark=get_daily_avg_sal(df_spark, \"salary_avg\")\n",
    "    \n",
    "    df_spark=get_year_month(df_spark,\"Posting Date\", \"Job_Post_Year\",\"Job_Post_Month\")\n",
    "    df_spark=get_year_month(df_spark,\"Process Date\", \"data_processed_year\",\"data_processed_month\")\n",
    "    \n",
    "    #Cleaning data by removing special characters in \"Preferred Skills\" column\n",
    "    df_spark=remove_special_chars(df_spark,\"Preferred Skills\")\n",
    "    \n",
    "    #One Hot encoding for Posting Type and Salary Frequency for converting Catgorical features to Numerical features\n",
    "    df_spark=one_hot_encoding(df_spark,'Posting Type')\n",
    "    df_spark=one_hot_encoding(df_spark,'Salary Frequency')\n",
    "    \n",
    "    #Feature Selection by removing the feautures with redundant data like, 'Posting Type','Salary Frequency', 'salary_avg'\n",
    "    df_spark=drop_feauture(df_spark,['Posting Type','Salary Frequency', 'salary_avg'])\n",
    "    \n",
    "    #writing data to csv file\n",
    "    write_data(df_spark,\"/dataset/output.csv\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-9904bd6a463a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m#Creating spark session variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;31m#extracting data from the provided file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mdf_spark\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/dataset/nyc-jobs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;31m# Categorical Imputation for filling missing 'Full-Time/Part-Time indicator' and 'Posting Date'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-9904bd6a463a>\u001b[0m in \u001b[0;36mfile_read\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# filename: File name with path to extract data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'True'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'escape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgap_filling_by_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_spark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrp_by_cols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreq_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
